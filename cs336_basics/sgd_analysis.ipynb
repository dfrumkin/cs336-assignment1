{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a362d7e1-37d1-4607-9594-941498b9d250",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections.abc import Callable\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "class SGD(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=1e-3):\n",
    "        if lr < 0:\n",
    "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
    "        defaults = {\"lr\": lr}\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure: Callable | None = None):\n",
    "        loss = None if closure is None else closure()\n",
    "        for group in self.param_groups:\n",
    "            lr = group[\"lr\"]  # Get the learning rate.\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "\n",
    "                state = self.state[p]  # Get state associated with p.\n",
    "                t = state.get(\"t\", 0)  # Get iteration number from the state, or initial value.\n",
    "                grad = p.grad.data  # Get the gradient of loss with respect to p.\n",
    "                p.data -= lr / math.sqrt(t + 1) * grad  # Update weight tensor in-place.\n",
    "                state[\"t\"] = t + 1  # Increment iteration number.\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884d4725-ae43-46e5-8e52-053dc802539f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "LEARNING_RATES = (1, 1e1, 1e2, 1e3)\n",
    "WEIGHTS = torch.nn.Parameter(5 * torch.randn((10, 10)))\n",
    "\n",
    "results = {}\n",
    "\n",
    "for lr in LEARNING_RATES:\n",
    "    weights = copy.deepcopy(WEIGHTS)\n",
    "    opt = SGD([weights], lr=lr)\n",
    "    results[lr] = []\n",
    "    \n",
    "    for t in range(100):\n",
    "        opt.zero_grad() # Reset the gradients for all learnable parameters.\n",
    "        loss = (weights**2).mean() # Compute a scalar loss value.\n",
    "        results[lr].append(loss.cpu().item())\n",
    "        loss.backward() # Run backward pass, which computes gradients.\n",
    "        opt.step() # Run optimizer step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e0b22b-291a-4e3e-9350-9e28d2cde39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.yscale(\"log\")\n",
    "for lr, losses in results.items():\n",
    "    plt.plot(range(len(losses)), losses, label=f\"lr={lr:g}\")\n",
    "\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss vs Iteration for different learning rates\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac088c4-6388-4264-8a9d-ecd30b2bc277",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[1000]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
