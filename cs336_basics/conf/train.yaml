defaults:
  - model: small
  - _self_
  - override hydra/job_logging: disabled
  - override hydra/hydra_logging: disabled

project: cs336-2025-a1
run_name: test  #??  # Provide at runtime

compile: true
# total_tokens_processed: 40_960_000  # MPS
total_tokens_processed: 327_680_000  # TinyStories
context_length: ${model.context_length}
batch_size: 32  # TinySories
lr: 3e-4
train_logging_freq: 10
val_logging_freq: 1000  # 200

wandb_id: null  # Set if continuing
start_checkpoint: null  # Set if continuing
checkpoint_dir: checkpoints

train_dataset: data/TinyStoriesV2-GPT4-train_tiny.tokens
valid_dataset: data/TinyStoriesV2-GPT4-valid_tiny.tokens

get_batch:
  _target_: cs336_basics.data.get_batch
  _partial_: true
  batch_size: ${batch_size}
  context_length: ${context_length} 
  device: ??

grad_clipping:
  _target_: cs336_basics.optimizer.gradient_clipping
  _partial_: true
  max_l2_norm: 1.0
  eps: 1e-6

optimizer:
  _target_: cs336_basics.optimizer.AdamW
  params: ??
  lr: ${lr}  # Not used, gets overwritten by scheduler
  betas: [0.9, 0.95]
  eps: 1e-8
  weight_decay: 1e-2

scheduler:
  _target_: cs336_basics.optimizer.get_lr_cosine_schedule
  _partial_: true
  max_learning_rate: ${lr}
  min_learning_rate: 3e-6
  warmup_iters: ??
  cosine_cycle_iters: ??

# Disable Hydra logging: we already have W&B for that
hydra:
  run:
    dir: .                # Don’t create outputs/<date>/<time>/
  output_subdir: null     # Don’t create .hydra/
